{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c4787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"trec06p-cs280\")\n",
    "data_dir.exists()\n",
    "\n",
    "ham_paths = []\n",
    "spam_paths = []\n",
    "\n",
    "with open(data_dir/Path(\"labels\"), \"r\") as label_file:\n",
    "    for line in label_file:\n",
    "        label, path = line.strip().split()\n",
    "        if label == 'ham':\n",
    "            ham_paths.append(path)\n",
    "        if label == 'spam':\n",
    "            spam_paths.append(path)\n",
    "\n",
    "\n",
    "# from ../data to trec06p-cs280/data na sea\n",
    "ham_paths = [path.replace('../', 'trec06p-cs280/', 1) for path in ham_paths] # the 1 means replace only the first occurrence \n",
    "spam_paths = [path.replace('../', 'trec06p-cs280/', 1) for path in spam_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60bad50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Since magulo yung encodings ng content ng mga files ng dataset, we use the function below to read the contents regardless sa encodings ng file.\n",
    "\"\"\"\n",
    "from charset_normalizer import from_path\n",
    "\n",
    "def read_file(path):\n",
    "    try:\n",
    "        result = from_path(path).best()\n",
    "        return str(result) if result is not None else None\n",
    "    except:\n",
    "        raise UnicodeDecodeError(f\"Unable to decode {path} using tried encodings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922d74a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for path in ham_paths:\n",
    "    try:\n",
    "        content = read_file(path)\n",
    "        if content is not None:\n",
    "            texts.append(content)\n",
    "            labels.append('ham')\n",
    "    except:\n",
    "        print(f\"Error reading file path: {path}\")\n",
    "\n",
    "for path in spam_paths:\n",
    "    try:\n",
    "        content = read_file(path)\n",
    "        if content is not None:\n",
    "            texts.append(content)\n",
    "            labels.append('spam')\n",
    "    except:\n",
    "        print(f\"Error reading file path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534c1826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37758, 37758)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts), len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1040d46",
   "metadata": {},
   "source": [
    "## Implement using numpy and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc4321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "ham_texts = []\n",
    "spam_texts = []\n",
    "\n",
    "for path in ham_paths:\n",
    "    try:\n",
    "        content = read_file(path)\n",
    "        if content is not None:\n",
    "            ham_texts.append(content)\n",
    "    except:\n",
    "        print(f\"Error reading file path: {path}\")\n",
    "\n",
    "for path in spam_paths:\n",
    "    try:\n",
    "        content = read_file(path)\n",
    "        if content is not None:\n",
    "            spam_texts.append(content)\n",
    "    except:\n",
    "        print(f\"Error reading file path: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0539d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test data\n",
    "ham_split_index = int(0.8*len(ham_texts))\n",
    "spam_split_index = int(0.8*len(spam_texts))\n",
    "\n",
    "train_ham_texts = ham_texts[0:ham_split_index]\n",
    "train_spam_texts = spam_texts[0:spam_split_index]\n",
    "\n",
    "test_ham_texts = ham_texts[ham_split_index:]\n",
    "test_spam_texts = spam_texts[spam_split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and tokenize documents\n",
    "def parse_documents(texts):\n",
    "    docs = []\n",
    "    for doc in texts:\n",
    "        tokens = re.findall(r'\\b[a-zA-Z]+\\b', doc)\n",
    "        tokens = [token.lower() for token in tokens] # lowercase dapat lahat\n",
    "        docs.append(tokens)\n",
    "    return docs\n",
    "\n",
    "parsed_ham_docs = parse_documents(train_ham_texts)\n",
    "parsed_spam_docs = parse_documents(train_spam_texts)\n",
    "\n",
    "\"\"\"\n",
    "returns something like this: [['you', 'have', 'won', 'money'], \n",
    "                              ['claim', 'your', 'money']]\n",
    "where each row is a single instance of a document\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac52ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137155, 59676)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique words/vocabulary of each class from the parsed documents\n",
    "\n",
    "def get_unique_tokens(tokenized_docs):\n",
    "    vocabulary = set()\n",
    "    for doc in tokenized_docs:\n",
    "        vocabulary.update(doc)\n",
    "    return vocabulary\n",
    "\n",
    "ham_vocabulary = get_unique_tokens(parsed_ham_docs)\n",
    "spam_vocabulary = get_unique_tokens(parsed_spam_docs)\n",
    "\n",
    "\"\"\"\n",
    "returns something like this: {'the', 'with', 'roan'} without any duplicates.\n",
    "\"\"\"\n",
    "\n",
    "len(ham_vocabulary), len(spam_vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3575eb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174387"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine ham_vocabulary and spam_vocabulary by using a union operator so that each elements are unique\n",
    "total_vocabulary = ham_vocabulary | spam_vocabulary\n",
    "len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0992d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137155, 59676)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count words and create a dict\n",
    "from collections import defaultdict\n",
    "ham_word_counts = defaultdict(int)\n",
    "spam_word_counts = defaultdict(int)\n",
    "\n",
    "\n",
    "for doc in parsed_ham_docs:\n",
    "    for word in doc:\n",
    "        if word in ham_vocabulary:\n",
    "            ham_word_counts[word] += 1\n",
    "\n",
    "for doc in parsed_spam_docs:\n",
    "    for word in doc:\n",
    "        if word in spam_vocabulary:\n",
    "            spam_word_counts[word] += 1\n",
    "\n",
    "\"\"\"\n",
    "returns something like this (word:count): {'the': 10, 'a': 5}\n",
    "\"\"\"\n",
    "len(ham_word_counts), len(spam_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a455339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the two dictionaries (ham and spam) by values\n",
    "ham_sorted_word_counts_dict = dict(sorted(ham_word_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "spam_sorted_word_counts_dict = dict(sorted(spam_word_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\"\"\"\n",
    "returns something like this (word:count): {'the': 10, 'a': 5, 'debby': 2} in descending order\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d1ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34188571806925777, 0.6581142819307423)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the prior probabilities of each Class\n",
    "prior_ham = len(parsed_ham_docs)/(len(parsed_ham_docs)+len(parsed_spam_docs))\n",
    "prior_spam = len(parsed_spam_docs)/(len(parsed_ham_docs)+len(parsed_spam_docs))\n",
    "\n",
    "prior_ham, prior_spam\n",
    "print(f\"Prior ham: {prior_ham}\")\n",
    "print(f\"Prior spam: {prior_spam}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac274dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>197468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>141665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edu</td>\n",
       "      <td>127371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>87422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>80018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137150</th>\n",
       "      <td>psicologia</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137151</th>\n",
       "      <td>hebron</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137152</th>\n",
       "      <td>pacifict</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137153</th>\n",
       "      <td>versitility</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137154</th>\n",
       "      <td>cantankerous</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137155 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word   count\n",
       "0                the  197468\n",
       "1                 to  141665\n",
       "2                edu  127371\n",
       "3                  a   87422\n",
       "4                 of   80018\n",
       "...              ...     ...\n",
       "137150    psicologia       1\n",
       "137151        hebron       1\n",
       "137152      pacifict       1\n",
       "137153   versitility       1\n",
       "137154  cantankerous       1\n",
       "\n",
       "[137155 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas dataframe for each class\n",
    "import pandas as pd\n",
    "\n",
    "ham_df = pd.DataFrame(list(ham_sorted_word_counts_dict.items()), columns=['word', 'count'])\n",
    "spam_df = pd.DataFrame(list(spam_sorted_word_counts_dict.items()), columns=['word', 'count'])\n",
    "\n",
    "ham_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9370d",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74142015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>count</th>\n",
       "      <th>P(word|ham)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>197468</td>\n",
       "      <td>3.249377e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>141665</td>\n",
       "      <td>2.331127e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>edu</td>\n",
       "      <td>127371</td>\n",
       "      <td>2.095916e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>87422</td>\n",
       "      <td>1.438547e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of</td>\n",
       "      <td>80018</td>\n",
       "      <td>1.316713e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137150</th>\n",
       "      <td>psicologia</td>\n",
       "      <td>1</td>\n",
       "      <td>1.645521e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137151</th>\n",
       "      <td>hebron</td>\n",
       "      <td>1</td>\n",
       "      <td>1.645521e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137152</th>\n",
       "      <td>pacifict</td>\n",
       "      <td>1</td>\n",
       "      <td>1.645521e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137153</th>\n",
       "      <td>versitility</td>\n",
       "      <td>1</td>\n",
       "      <td>1.645521e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137154</th>\n",
       "      <td>cantankerous</td>\n",
       "      <td>1</td>\n",
       "      <td>1.645521e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>137155 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word   count   P(word|ham)\n",
       "0                the  197468  3.249377e-02\n",
       "1                 to  141665  2.331127e-02\n",
       "2                edu  127371  2.095916e-02\n",
       "3                  a   87422  1.438547e-02\n",
       "4                 of   80018  1.316713e-02\n",
       "...              ...     ...           ...\n",
       "137150    psicologia       1  1.645521e-07\n",
       "137151        hebron       1  1.645521e-07\n",
       "137152      pacifict       1  1.645521e-07\n",
       "137153   versitility       1  1.645521e-07\n",
       "137154  cantankerous       1  1.645521e-07\n",
       "\n",
       "[137155 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate each probability of each word given a class P(word | class)\n",
    "\n",
    "total_ham_words = ham_df['count'].sum()\n",
    "total_spam_words = spam_df['count'].sum()\n",
    "\n",
    "# PS. Wala pang laplace smoothing dito kung meron man, ganito magiging itsura: ham_df['P(word|ham)'] = (ham_df['count'] + alpha) / (total_ham_words + alpha * len(ham_vocabulary))\n",
    "ham_df['P(word|ham)'] = (ham_df['count']) / total_ham_words\n",
    "spam_df['P(word|spam)'] = spam_df['count'] / total_spam_words\n",
    "\n",
    "ham_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e98d6",
   "metadata": {},
   "source": [
    "## Create a Function to Predict the Class of a New Document\n",
    "\n",
    "Note: use log to avoid underflow from multiplying many small probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a059f7",
   "metadata": {},
   "source": [
    "### Without laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3ce4affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Convert prob column to a dict for fast lookup\n",
    "ham_probs = dict(zip(ham_df['word'], ham_df['P(word|ham)']))\n",
    "spam_probs = dict(zip(spam_df['word'], spam_df['P(word|spam)']))\n",
    "\n",
    "\n",
    "# Without laplace smoothing\n",
    "def predict(input_document_tokens, prior_ham, prior_spam):\n",
    "    log_prob_ham = math.log(prior_ham)\n",
    "    log_prob_spam = math.log(prior_spam)\n",
    "\n",
    "    total_ham_probs = log_prob_ham\n",
    "    total_spam_probs =log_prob_spam\n",
    "\n",
    "    \n",
    "    for input_word in input_document_tokens:\n",
    "        # Calculate for total_ham_probs\n",
    "        if input_word in ham_probs:\n",
    "            total_ham_probs += math.log(ham_probs[input_word])\n",
    "        else:\n",
    "            total_ham_probs += 0\n",
    "\n",
    "        # Calculate for total_spam_probs\n",
    "        if input_word in spam_probs:\n",
    "            total_spam_probs += math.log(spam_probs[input_word])\n",
    "        else:\n",
    "            total_spam_probs += 0\n",
    "    \n",
    "    return 'ham' if total_ham_probs>total_spam_probs else 'spam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6555d8",
   "metadata": {},
   "source": [
    "### With laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a99dfdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With laplace smoothing\n",
    "\n",
    "alpha = 0.0000005\n",
    "V = len(total_vocabulary)\n",
    "\n",
    "ham_df['P(word|ham)'] = (ham_df['count']+alpha) / (total_ham_words + alpha*V)\n",
    "spam_df['P(word|spam)'] = (spam_df['count']+alpha) / (total_spam_words + alpha*V)\n",
    "\n",
    "ham_probs = dict(zip(ham_df['word'], ham_df['P(word|ham)']))\n",
    "spam_probs = dict(zip(spam_df['word'], spam_df['P(word|spam)']))\n",
    "\n",
    "def predict_with_laplace_smoothing(input_document_tokens, prior_ham, prior_spam):\n",
    "    log_prob_ham = math.log(prior_ham)\n",
    "    log_prob_spam = math.log(prior_spam)\n",
    "\n",
    "    total_ham_probs = log_prob_ham\n",
    "    total_spam_probs =log_prob_spam\n",
    "\n",
    "    for input_word in input_document_tokens:\n",
    "        # Calculate for total_ham_probs\n",
    "        if input_word in ham_probs:\n",
    "            total_ham_probs += math.log(ham_probs[input_word])\n",
    "        else:\n",
    "            smoothed_ham = (alpha) / (total_ham_words + alpha * V)\n",
    "            total_ham_probs += math.log(smoothed_ham)\n",
    "\n",
    "        # Calculate for total_spam_probs\n",
    "        if input_word in spam_probs:\n",
    "            total_spam_probs += math.log(spam_probs[input_word])\n",
    "        else:\n",
    "            smoothed_spam = (alpha) / (total_spam_words + alpha * V)\n",
    "            total_spam_probs += math.log(smoothed_spam)\n",
    "    \n",
    "    return 'ham' if total_ham_probs>total_spam_probs else 'spam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49a016b",
   "metadata": {},
   "source": [
    "### Evaluation with laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1e56347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "# Combine test texts\n",
    "X_test = test_ham_texts + test_spam_texts\n",
    "y_test = ['ham'] * len(test_ham_texts) + ['spam'] * len(test_spam_texts)\n",
    "\n",
    "# Parse and tokenize the document:\n",
    "parsed_input_docs = parse_documents(X_test)\n",
    "\n",
    "# Predict multiple documents\n",
    "def predict_documents(documents):\n",
    "    return [predict_with_laplace_smoothing(document, prior_ham, prior_spam) for document in documents]\n",
    "\n",
    "y_pred = predict_documents(parsed_input_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb7da07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "# flatten/reshape the parsed_input_doc:\n",
    "parsed_input_doc = parse_documents([\"\"\"Congratulations!\n",
    "\n",
    "Youâ€™ve been chosen to receive an exclusive **$1000 Walmart Gift Card** â€” absolutely FREE! ðŸŽ‰  \n",
    "But hurry, this offer is only valid for the next 24 hours!\n",
    "\n",
    "ðŸ‘‰ Click here to claim your reward now: [http://claim-your-prize-now.xyz](http://claim-your-prize-now.xyz)  \n",
    "(No purchase necessary!)\n",
    "\n",
    "Act fast before this opportunity disappears forever.\n",
    "\n",
    "Best wishes,  \n",
    "The Rewards Team\n",
    "\n",
    "P.S. This is a limited-time offer only available to select customers.\n",
    "\"\"\"])\n",
    "\n",
    "flattened_parsed_input_doc = parsed_input_doc[0]\n",
    "\n",
    "predict_with_laplace_smoothing(flattened_parsed_input_doc, prior_ham, prior_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edf153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9401483050847458\n",
      "Precision: 0.9745798319327731\n",
      "Recall: 0.933400402414487\n",
      "F1 Score: 0.9535457348406988\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.88      0.95      0.92      2582\n",
      "        spam       0.97      0.93      0.95      4970\n",
      "\n",
      "    accuracy                           0.94      7552\n",
      "   macro avg       0.93      0.94      0.93      7552\n",
      "weighted avg       0.94      0.94      0.94      7552\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2461,  121],\n",
       "       [ 331, 4639]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# # Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['ham', 'spam'])\n",
    "\n",
    "\n",
    "# Output\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, pos_label='spam'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, pos_label='spam'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, pos_label='spam'))\n",
    "\n",
    "# Full classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15982556",
   "metadata": {},
   "source": [
    "# Putting it all together using Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca64187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\"\n",
    "        Initializes the classifier with a smoothing parameter lambda.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.ham_word_counts_dict = defaultdict(int) # word counts dictionary for ham class\n",
    "        self.spam_word_counts_dict = defaultdict(int) # word counts dictionary for spam class\n",
    "        self.ham_num_doc = 0\n",
    "        self.spam_num_doc = 0\n",
    "        self.ham_vocabulary = set()  # combined vocabulary for ham\n",
    "        self.spam_vocabulary = set()  # combined vocabulary for spam\n",
    "        self.vocabulary = set()  # combined vocabulary across classes\n",
    "        self.fitted = False\n",
    "\n",
    "    def fit(self, ham_docs, spam_docs):\n",
    "        # Update document counts\n",
    "        self.ham_doc_count = len(ham_docs)\n",
    "        self.spam_doc_count = len(spam_docs)\n",
    "        \n",
    "        # Get vocabularies dictionaries (word:count) of each class\n",
    "        # ham_docs and spam_docs are already tokenized/parsed ma dapat\n",
    "        for doc in ham_docs:\n",
    "            for word in doc:\n",
    "                self.ham_word_counts_dict[word] += 1\n",
    "                self.vocabulary.add(word)\n",
    "                self.ham_vocabulary.add(word)\n",
    "        for doc in spam_docs:\n",
    "            for word in doc:\n",
    "                self.spam_word_counts_dict[word] += 1\n",
    "                self.vocabulary.add(word)\n",
    "                self.spam_vocabulary.add(word)\n",
    "\n",
    "        # Total words per class\n",
    "        self.total_ham_words = sum(self.ham_word_counts_dict.values())\n",
    "        self.total_spam_words = sum(self.spam_word_counts_dict.values())\n",
    "\n",
    "        # Total words for all classes (ginagamit usually for smoothing itong si V)\n",
    "        self.V = len(self.vocabulary)\n",
    "\n",
    "        # Compute prior probabilities\n",
    "        self.prior_ham = self.ham_doc_count/(self.ham_doc_count+self.spam_doc_count)\n",
    "        self.prior_spam = self.ham_doc_count/(self.ham_doc_count+self.spam_doc_count)\n",
    "\n",
    "        self.fitted = True\n",
    "\n",
    "    def _calc_word_prob(self, word, class_word_counts, total_words):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the smoothed likelihood of a word \n",
    "        given a class.\n",
    "        \n",
    "        Parameters:\n",
    "        word (str): The word to calculate probability for.\n",
    "        class_word_counts (dict): Word Dictionary with counts for the given class.\n",
    "        total_words (int): Total word count for the given class.\n",
    "        \n",
    "        Returns:\n",
    "        laplac smoothed probability P(word|class).\n",
    "        \"\"\"\n",
    "        word_count = class_word_counts[word]\n",
    "        # Laplace smoothing: count + alpha divided by total words plus alpha * vocabulary size\n",
    "        return (word_count + self.alpha) / (total_words + self.alpha * self.V)\n",
    "\n",
    "        \n",
    "    def get_unique_tokens(tokenized_docs):\n",
    "        vocabulary = set()\n",
    "        for doc in tokenized_docs:\n",
    "            vocabulary.update(doc)\n",
    "        return vocabulary\n",
    "    \n",
    "    def predict(self, input_document_token):\n",
    "        \"\"\"\n",
    "        Predict the class ('ham' or 'spam') for a new document.\n",
    "        \n",
    "        Parameters:\n",
    "        document_tokens (list of str): The tokenized document.\n",
    "        \n",
    "        Returns:\n",
    "        str: Predic\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise Exception(\"Classifier not trained yet. Call fit() first.\")\n",
    "        \n",
    "        log_prob_ham = math.log(self.prior_ham)\n",
    "        log_prob_spam = math.log(self.prior_spam)\n",
    "\n",
    "        total_ham_probs = log_prob_ham\n",
    "        total_spam_probs =log_prob_spam\n",
    "\n",
    "        for input_word in input_document_token:\n",
    "            # Calculate for total_ham_probs\n",
    "            if input_word in self.ham_word_counts_dict:\n",
    "                word_ham_prob_smoothed = self._calc_word_prob(input_word, self.ham_word_counts_dict, self.total_ham_words)\n",
    "                total_ham_probs += math.log(word_ham_prob_smoothed)\n",
    "            else:\n",
    "                smoothed_ham = (self.alpha) / (self.total_ham_words + self.alpha * self.V)\n",
    "                total_ham_probs += math.log(smoothed_ham)\n",
    "\n",
    "            # Calculate for total_spam_probs\n",
    "            if input_word in self.spam_word_counts_dict:\n",
    "                word_spam_prob_smoothed = self._calc_word_prob(input_word, self.spam_word_counts_dict, self.total_spam_words)\n",
    "                total_spam_probs += math.log(word_spam_prob_smoothed)\n",
    "            else:\n",
    "                smoothed_spam = (self.alpha) / (total_spam_words + self.alpha * self.V)\n",
    "                total_spam_probs += math.log(smoothed_spam)\n",
    "        \n",
    "        return 'ham' if total_ham_probs>total_spam_probs else 'spam'\n",
    "    \n",
    "    def predict_many(self, input_document_tokens):\n",
    "        return [self.predict(doc) for doc in input_document_tokens]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4feabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with some random lambda vfalue\n",
    "model = NaiveBayesClassifier(alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0169fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/fit the model\n",
    "model.fit(parsed_ham_docs, parsed_spam_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0bcb3918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "# flatten/reshape the parsed_input_doc:\n",
    "parsed_input_doc = parse_documents([\"\"\"Congratulations!\n",
    "\n",
    "Youâ€™ve been chosen to receive an exclusive **$1000 Walmart Gift Card** â€” absolutely FREE! ðŸŽ‰  \n",
    "But hurry, this offer is only valid for the next 24 hours!\n",
    "\n",
    "ðŸ‘‰ Click here to claim your reward now: [http://claim-your-prize-now.xyz](http://claim-your-prize-now.xyz)  \n",
    "(No purchase necessary!)\n",
    "\n",
    "Act fast before this opportunity disappears forever.\n",
    "\n",
    "Best wishes,  \n",
    "The Rewards Team\n",
    "\n",
    "P.S. This is a limited-time offer only available to select customers.\n",
    "\"\"\"])\n",
    "\n",
    "flattened_parsed_input_doc = parsed_input_doc[0]\n",
    "\n",
    "model.predict(flattened_parsed_input_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5c1d3d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9401483050847458\n",
      "Precision: 0.9745798319327731\n",
      "Recall: 0.933400402414487\n",
      "F1 Score: 0.9535457348406988\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.88      0.95      0.92      2582\n",
      "        spam       0.97      0.93      0.95      4970\n",
      "\n",
      "    accuracy                           0.94      7552\n",
      "   macro avg       0.93      0.94      0.93      7552\n",
      "weighted avg       0.94      0.94      0.94      7552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Combine test texts\n",
    "X_test = test_ham_texts + test_spam_texts\n",
    "y_test = ['ham'] * len(test_ham_texts) + ['spam'] * len(test_spam_texts)\n",
    "\n",
    "# Parse and tokenize the document:\n",
    "parsed_input_docs = parse_documents(X_test)\n",
    "\n",
    "\n",
    "y_preds = model.predict_many(parsed_input_docs)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['ham', 'spam'])\n",
    "\n",
    "# Output\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, pos_label='spam'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, pos_label='spam'))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, pos_label='spam'))\n",
    "\n",
    "# Optional: Full classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# # Report\n",
    "# report = classification_report(y_true, y_pred, labels=['ham', 'spam'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0657f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluation Metrics ===\n",
    "\n",
    "def compute_precision_recall(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes precision and recall given true labels and predictions.\n",
    "    We assume:\n",
    "        - TP: spam correctly classified as spam.\n",
    "        - TN: ham correctly classified as ham.\n",
    "        - FP: ham misclassified as spam.\n",
    "        - FN: spam misclassified as ham.\n",
    "    \"\"\"\n",
    "    tp = sum(1 for true, pred in zip(y_true, y_pred) if true == 'spam' and pred == 'spam')\n",
    "    fp = sum(1 for true, pred in zip(y_true, y_pred) if true == 'ham' and pred == 'spam')\n",
    "    fn = sum(1 for true, pred in zip(y_true, y_pred) if true == 'spam' and pred == 'ham')\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aa07cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 2.0 -> Precision: 0.9550, Recall: 0.9052\n",
      "Lambda: 1.0 -> Precision: 0.9600, Recall: 0.9119\n",
      "Lambda: 0.5 -> Precision: 0.9622, Recall: 0.9167\n",
      "Lambda: 0.1 -> Precision: 0.9661, Recall: 0.9294\n",
      "Lambda: 0.005 -> Precision: 0.9697, Recall: 0.9352\n"
     ]
    }
   ],
   "source": [
    "# Parse and tokenize the document:\n",
    "parsed_test_docs = parse_documents(X_test)\n",
    "\n",
    "def evaluate_with_lambdas(lambdas, parsed_ham_docs, parsed_spam_docs, test_docs, true_labels):\n",
    "    results = {}\n",
    "\n",
    "    for lam in lambdas:\n",
    "        # Create a new classifier instance with the given lambda (smoothing parameter)\n",
    "        nb_temp = NaiveBayesClassifier(alpha=lam)\n",
    "        nb_temp.fit(parsed_ham_docs, parsed_spam_docs)  # using tokenized training documents\n",
    "        \n",
    "        # Predict on test set (assuming test_docs and true_labels are defined)\n",
    "        predicted = nb_temp.predict_many(test_docs)\n",
    "        prec, rec = compute_precision_recall(true_labels, predicted)\n",
    "        \n",
    "        results[lam] = {'precision': prec, 'recall': rec}\n",
    "        print(f\"Lambda: {lam} -> Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
    "\n",
    "\n",
    "lambdas = [2.0, 1.0, 0.5, 0.1, 0.005]\n",
    "evaluate_with_lambdas(lambdas, parsed_ham_docs, parsed_spam_docs, parsed_test_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4922f162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top informative words for spam:\n",
      "[('shopzilla', 11.43542166732738), ('greylink', 10.847642252650473), ('hoodia', 10.808383040199798), ('ephedra', 10.67150372415245), ('dgts', 10.624502779564844), ('edua', 10.60888420515285), ('anotherparty', 10.514238662240045), ('vipcollectvip', 10.482784437625622), ('writely', 10.470491429223157), ('yfti', 10.40766564015281)]\n",
      "Top informative words for ham:\n",
      "[('lugnet', -12.055427248315494), ('cert', -11.62611846471917), ('rpcss', -11.3925960721898), ('pdfzone', -11.27937657794656), ('pnfs', -11.194554718818175), ('patched', -11.189624633730942), ('reqs', -10.765040837629398), ('csf', -10.578989505723465), ('jforster', -10.548806437797001), ('mrc', -10.521547293806618)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def informative_words(classifier:NaiveBayesClassifier, top_n=200):\n",
    "    \"\"\"\n",
    "    Computes the informative score (log-odds) for each word and returns the top_n words.\n",
    "    Positive score: indicative of spam.\n",
    "    Negative score: indicative of ham.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for word in classifier.vocabulary:\n",
    "        # Calculate P(word|class)\n",
    "        p_spam = (classifier.spam_word_counts_dict[word] + classifier.alpha) / (classifier.total_spam_words + classifier.alpha * classifier.V)\n",
    "        p_ham  = (classifier.ham_word_counts_dict[word]  + classifier.alpha)  / (classifier.total_ham_words  + classifier.alpha * classifier.V)\n",
    "        # Compute log odds ratio\n",
    "        score = math.log(p_spam / p_ham) # positive if p_spam is bigger (the word is more likely to be a spam), negative if p_ham is bigger (the word is more likely to be a ham)\n",
    "        scores[word] = score\n",
    "    # Sort words by the magnitude of the log odds ratio (most informative)\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: abs(x[1]), reverse=True)  # Sort by the absolute value of the score\n",
    "    return sorted_words[:top_n]\n",
    "\n",
    "# Use the best lambda from above \n",
    "nb_best = NaiveBayesClassifier(alpha=0.05)\n",
    "nb_best.fit(parsed_ham_docs, parsed_spam_docs)\n",
    "\n",
    "# get the top 200 informative words along with their scores.\n",
    "top_informative = informative_words(nb_best, top_n=200)\n",
    "\n",
    "# Separate into spam and ham lists:\n",
    "spam_informative = [(word, score) for word, score in top_informative if score > 0]\n",
    "ham_informative  = [(word, score) for word, score in top_informative if score < 0]\n",
    "\n",
    "print(\"Top informative words for spam:\")\n",
    "print(spam_informative[:10])  # printing top 10 \n",
    "print(\"Top informative words for ham:\")\n",
    "print(ham_informative[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "511a00e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using only informative words -> Precision: 0.8008, Recall: 0.9895\n"
     ]
    }
   ],
   "source": [
    "# Create a set of the top 200 informative words\n",
    "informative_vocabulary = set([word for word, score in top_informative])\n",
    "\n",
    "# Modifyng the prediction function to ignore words not in informative_vocabulary:\n",
    "def predict_with_informative(classifier, document_tokens, informative_vocabulary):\n",
    "    # Only consider tokens that are in the informative vocabulary\n",
    "    filtered_tokens = [word for word in document_tokens if word in informative_vocabulary]\n",
    "    return classifier.predict(filtered_tokens)\n",
    "\n",
    "# Evaluate on the test set using the filtered vocabulary:\n",
    "def predict_many_informative(classifier, documents, informative_vocabulary):\n",
    "    return [predict_with_informative(classifier, doc, informative_vocabulary) for doc in documents]\n",
    "\n",
    "predicted_informative = predict_many_informative(nb_best, parsed_test_docs, informative_vocabulary)\n",
    "prec_inf, rec_inf = compute_precision_recall(y_test, predicted_informative)\n",
    "print(\"Using only informative words -> Precision: {:.4f}, Recall: {:.4f}\".format(prec_inf, rec_inf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad44b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
